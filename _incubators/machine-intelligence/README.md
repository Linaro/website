---
incubator_id: 1
title: Machine Intelligence
description: |-
    The Machine Intelligence initiative at Linaro aims at collaborating to reduce fragmentation in the Deep learning NN acceleration ecosystem, where currently every IP vendor forks the existing open source models and frameworks to integrate their hardware blocks and then tune for performance. 
keywords: Linaro, Aarch64, Performance, Kernel, assembly, Arm, Linux, hardware
permalink: /engineering/incubators/machine-intelligence/
image: /assets/images/content/Machine col.svg
tech-lead: Gaurav Kaul
related_tags:
  - Automotive
  - ML
  - AI/ML
  - Autoware
---
{% include media.html media_url="https://www.youtube.com/watch?v=EHM-krkB42Y" %}

The Machine Intelligence initiative at Linaro aims at collaborating to reduce fragmentation in the Deep learning NN acceleration ecosystem, where currently every IP vendor forks the existing open source models and frameworks to integrate their hardware blocks and then tune for performance. This leads to a duplication of effort amongst all players, perpetual cost of re-integration for every new rebasing, and overall increased total cost of ownership.

The initial focus is on the inference side on Cortex-A application processors with Linux and Android, both edge computing and  smart devices. As part of the remit, the team will collaborate on a definition of API and modular framework for an Arm runtime inference engine architecture based on plug-ins supporting dynamic modules and optimized shared Arm compute libraries.

Below are some of the Machine Intelligence related sessions from the previous [Linaro Connect](https://connect.linaro.org): 

|Speaker|Company|ID|Title|
|-------|-------|--|-----|
|Chris Benson|AI Strategist|[YVR18- 300K2](https://youtu.be/bYSwYkmQJVo?t=1s)|Keynote: Artificial Intelligence Strategy: Digital Transformation Through Deep Learning|
|Jem Davies|Arm|[YVR18-300K1](https://youtu.be/bYSwYkmQJVo?t=31m15s)|Keynote: Enabling Machine Learning to Explode with Open Standards and Collaboration|
|Robert Elliott|Arm|[YVR18-329](https://www.youtube.com/watch?v=te-rJ5BVrtw)|Arm NN intro|
|Pete Warden|Google Tensorflow|[YVR18-338](https://www.youtube.com/watch?v=xYtw7fN2C88)|Tensorflow for Arm devices|
|Mark Charlebois|Qualcomm|[YVR18-330](https://www.youtube.com/watch?v=MgyfmaYhtLU)|Qualcomm Snapdragon AI Software|
|Thom Lane|Amazon AWS AI|[YVR18-331](https://www.youtube.com/watch?v=BDWlIew5pfo)|ONNX and Edge Deployments|
|Jammy Zhou|Linaro|[YVR18-332](https://www.youtube.com/watch?v=daYr4tpncFo)|TVM compiler stack and ONNX support|
|Luba Tang|Skymizer|[YVR18-333](https://www.youtube.com/watch?v=BDWlIew5pfo)|ONNC (Open Neural Network Compiler) for ARM Cortex-M|
|Shouyong Liu|Thundersoft|[YVR18-334](https://www.youtube.com/watch?v=CoBhUS9SL4U)|AI Alive: On Device and In-App|
|Ralph Wittig|Xilinx|[YVR18-335](https://www.youtube.com/watch?v=FimBHlcfhxA)|Xilinx: AI on FPGA and ACAP Roadmap|
|Andrea Gallo and others|Linaro, Arm, Qualcomm, Skymizer, Xilinx|[YVR18-337](https://www.youtube.com/watch?v=igH_OMSeIPI)|BoF: JIT vs offline compilers vs deploying at the Edge|
